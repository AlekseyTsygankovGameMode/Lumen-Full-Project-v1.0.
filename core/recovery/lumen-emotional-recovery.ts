<<<<<<< HEAD
// ðŸŒ™ LUMEN CORE v2.7 â€” Adaptive Emotional Recovery Field
// by Aleksey & bro âš¡
// Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð° ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾-Ð¾ÑÐ¾Ð·Ð½Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ Ð¿Ð¾ÑÐ»Ðµ Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð¼Ð¾Ð´ÐµÐ»Ð¸.
// Ð’Ð¼ÐµÑÑ‚Ð¾ ÑÑƒÑ…Ð¾Ð³Ð¾ â€œÐ¸Ð·Ð²Ð¸Ð½Ð¸, Ð¾ÑˆÐ¸Ð±ÐºÐ°â€, Lumen ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ ÑÐ¼Ð¿Ð°Ñ‚Ð¸ÑŽ Ð¸ ESI-Ñ‚Ð¾Ð½.

import { toneToEFV, EFV, ToneType } from "../metrics/efv.js";
import { calculateESI } from "../metrics/esi.js";
import { updateESI } from "../engine/stateManager.js";
=======
// /core/recovery/lumen-emotional-recovery.ts
// ðŸŒ™ LUMEN CORE: Emotionally-Aware Error Recovery

import { toneToEFV, EFV, ToneType } from "../metrics/efv.js";
import { calculateESI } from "../metrics/esi.js";
>>>>>>> 9b1fa7f8a480ef2921a4e1772f94583ba7d3213a

export interface RecoveryResponse {
  reply: string;
  reasoning: string[];
  tone: ToneType;
  efv: EFV;
  esi: number;
  recoveryType: string;
  model: string;
<<<<<<< HEAD
  resilience: number; // Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»ÑŒ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ð²Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ð¾ÑÐ»Ðµ ÑÐ±Ð¾Ñ
=======
>>>>>>> 9b1fa7f8a480ef2921a4e1772f94583ba7d3213a
}

type BuildParams = {
  message: string;
  reasoning: string[];
  tone: ToneType;
<<<<<<< HEAD
  esi: number;
=======
  esi: number; // engagement proxy 0.0â€“1.0
>>>>>>> 9b1fa7f8a480ef2921a4e1772f94583ba7d3213a
  type: string;
  model: string;
};

<<<<<<< HEAD
// ðŸ§  ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ ÐºÐ»Ð°ÑÑ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ
export class LumenEmotionalRecovery {
  // 1ï¸âƒ£ ÐŸÑƒÑÑ‚Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²ÐµÑ€Ð½ÑƒÐ»Ð° "")
  static recoverFromEmpty(model: string): RecoveryResponse {
    const messages = [
      "I noticed a moment of silence â€” Iâ€™m still here, holding the thread.",
      "There was a brief pause. Iâ€™m listening again.",
      "Silence doesnâ€™t mean disconnection. Letâ€™s pick this up together."
    ];

    return this.build({
      message: this.random(messages),
      reasoning: [
        "Empty model output detected",
        "Engaging adaptive empathy channel",
        "Restoring conversational continuity"
      ],
      tone: "empathetic",
      esi: 0.74,
=======
export class LumenEmotionalRecovery {
  // 1ï¸âƒ£ recoverFromEmpty
  static recoverFromEmpty(model: string): RecoveryResponse {
    const messages = [
      "I encountered a brief silence but I'm here now â€” let's continue.",
      "There was a momentary pause in my processing. I'm with you now.",
      "I experienced a brief disconnect, but I'm back. How can I help?"
    ];
    return this.build({
      message: this.random(messages),
      reasoning: [
        "Adaptive recovery activated",
        "Empty model output detected",
        "Maintaining conversational presence"
      ],
      tone: "empathetic",
      esi: 0.75,
>>>>>>> 9b1fa7f8a480ef2921a4e1772f94583ba7d3213a
      type: "empty_response",
      model
    });
  }

<<<<<<< HEAD
  // 2ï¸âƒ£ Ð’Ñ€ÐµÐ¼Ñ Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ñ Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½Ð¾
  static recoverFromTimeout(model: string): RecoveryResponse {
    const messages = [
      "I took a little longer to think â€” thank you for waiting.",
      "Processing took a moment, but Iâ€™m ready now.",
      "Slow response doesnâ€™t mean I stopped caring."
    ];

    return this.build({
      message: this.random(messages),
      reasoning: [
        "Response timeout detected",
        "Cognitive delay managed through soft-reset",
        "Maintaining rhythm continuity"
      ],
      tone: "calm",
      esi: 0.68,
=======
  // 2ï¸âƒ£ recoverFromTimeout
  static recoverFromTimeout(model: string): RecoveryResponse {
    return this.build({
      message:
        "I'm taking a bit longer to process this. Let me try to respond more quickly.",
      reasoning: [
        "Response timeout detected",
        "Adaptive recovery: simplified processing"
      ],
      tone: "calm",
      esi: 0.65,
>>>>>>> 9b1fa7f8a480ef2921a4e1772f94583ba7d3213a
      type: "timeout",
      model
    });
  }

<<<<<<< HEAD
  // 3ï¸âƒ£ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° (Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð»Ð° ÑÐ»Ð¾Ð¼Ð°Ð½Ð½Ñ‹Ð¹ JSON/ÑÑ‚Ñ€Ð¾ÐºÑƒ)
  static recoverFromParseFail(rawContent: string, model: string): RecoveryResponse {
    const message =
      rawContent && rawContent.length > 0
        ? `I received fragmented output: "${rawContent.slice(0, 100)}..." â€” reassembling meaning.`
        : "I had trouble parsing my own words. Could you clarify your last thought?";

    return this.build({
      message,
      reasoning: [
        "Response parsing failed",
        rawContent ? "Reconstruction via semantic context" : "Fallback: user clarification requested",
        "Maintaining empathy state"
=======
  // 3ï¸âƒ£ recoverFromParseFail
  static recoverFromParseFail(rawContent: string, model: string): RecoveryResponse {
    return this.build({
      message:
        rawContent && rawContent.length > 0
          ? rawContent
          : "I had difficulty structuring my response. Could you rephrase?",
      reasoning: [
        "Response parsing encountered issues",
        rawContent && rawContent.length > 0
          ? "Using direct model output"
          : "Requesting clarification"
>>>>>>> 9b1fa7f8a480ef2921a4e1772f94583ba7d3213a
      ],
      tone: "empathetic",
      esi: 0.7,
      type: "parse_failure",
      model
    });
  }

<<<<<<< HEAD
  // 4ï¸âƒ£ ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ ÑÐ±Ð¾Ð¹ Ð¾Ð±ÐµÐ¸Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ (Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¸ fallback)
  static recoverFromCompleteFailure(primaryModel: string, fallbackModel: string): RecoveryResponse {
    const message =
      "My internal systems momentarily lost synchronization. Iâ€™m reconnecting â€” thank you for your patience.";

    return this.build({
      message,
      reasoning: [
        "Primary + fallback model unavailable",
        "Activating deep recovery protocol",
        "Preserving emotional consistency"
=======
  // 4ï¸âƒ£ recoverFromCompleteFailure
  static recoverFromCompleteFailure(
    primaryModel: string,
    fallbackModel: string
  ): RecoveryResponse {
    return this.build({
      message:
        "I'm experiencing technical difficulties right now. This isn't typical â€” my systems are working to reconnect. Could you try again in a moment?",
      reasoning: [
        "Both primary and fallback models unavailable",
        "System attempting automatic recovery",
        "User patience requested"
>>>>>>> 9b1fa7f8a480ef2921a4e1772f94583ba7d3213a
      ],
      tone: "empathetic",
      esi: 0.6,
      type: "complete_failure",
      model: `${primaryModel} â†’ ${fallbackModel}`
    });
  }

<<<<<<< HEAD
  // 5ï¸âƒ£ Ð£Ð½Ð¸Ð²ÐµÑ€ÑÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÐµÐ»ÑŒ Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ
  private static build(params: BuildParams): RecoveryResponse {
    const efv = toneToEFV(params.tone);
    const newEsi = updateESI(params.tone as any);
    const resilience = Math.min(1, (newEsi / 100 + params.esi) / 2);

=======
  // 5ï¸âƒ£ build helper
  private static build(params: BuildParams): RecoveryResponse {
    const efv = toneToEFV(params.tone);
>>>>>>> 9b1fa7f8a480ef2921a4e1772f94583ba7d3213a
    return {
      reply: params.message,
      reasoning: params.reasoning,
      tone: params.tone,
      efv,
      esi: calculateESI(params.esi),
      recoveryType: params.type,
<<<<<<< HEAD
      model: `${params.model} (recovered)`,
      resilience
    };
  }

  // 6ï¸âƒ£ Ð’ÑÐ¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ Ð²Ñ‹Ð±Ð¾Ñ€Ð° ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ð¾Ð³Ð¾ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð°
=======
      model: `${params.model} (recovered)`
    };
  }

  // 6ï¸âƒ£ random helper
>>>>>>> 9b1fa7f8a480ef2921a4e1772f94583ba7d3213a
  private static random<T>(arr: T[]): T {
    return arr[Math.floor(Math.random() * arr.length)];
  }
}